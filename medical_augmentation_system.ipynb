# Информационная система для аугментации медицинских изображений
# с использованием DCGAN, DDPM и StyleGAN

# Подготовлено для работы на Google Colab Pro с GPU A100

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset, Dataset
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import make_grid, save_image
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import time
import random
from google.colab import drive
from IPython.display import clear_output, display
import warnings
warnings.filterwarnings("ignore")

# Устанавливаем переменную окружения для синхронной отладки CUDA
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# Проверка доступности GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Используемое устройство: {device}")

# Проверка типа GPU
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    print(f"Модель GPU: {gpu_name}")
    print(f"Количество доступных GPU: {torch.cuda.device_count()}")
    print(f"Доступная память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} ГБ")

# Отключаем cudnn для GroupNorm (можно включить после проверки)
torch.backends.cudnn.enabled = False
print("cuDNN отключен для устранения ошибок в GroupNorm.")

# Создаем директорию для данных MedMNIST
DATA_DIR = './medmnist_data'
os.makedirs(DATA_DIR, exist_ok=True)
print(f"Создана директория для данных MedMNIST: {DATA_DIR}")

# Установка seed для воспроизводимости результатов
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True

# Подключение Google Drive
drive.mount('/content/drive')

# Создаем директории для сохранения моделей и результатов
RESULTS_DIR = '/content/drive/MyDrive/med_image_augmentation'
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(f"{RESULTS_DIR}/dcgan", exist_ok=True)
os.makedirs(f"{RESULTS_DIR}/ddpm", exist_ok=True)
os.makedirs(f"{RESULTS_DIR}/stylegan", exist_ok=True)
os.makedirs(f"{RESULTS_DIR}/evaluation", exist_ok=True)

# Установка необходимых пакетов
!pip install medmnist gradio scikit-image

## 1. Загрузка и подготовка данных MedMNIST

import medmnist
from medmnist import INFO

class FilteredDataLoader:
    def __init__(self, dataloader):
        self.dataloader = dataloader

    def __iter__(self):
        for batch in self.dataloader:
            if batch[0].size(0) > 0:
                yield batch
            else:
                print("Пропущен пустой батч в FilteredDataLoader")

    def __len__(self):
        return len(self.dataloader)

def validate_dataset(dataset):
    valid_indices = []
    for idx in range(len(dataset)):
        try:
            img, label = dataset[idx]
            if img is not None and label is not None:
                valid_indices.append(idx)
        except Exception as e:
            print(f"Ошибка при загрузке индекса {idx}: {e}")
    return Subset(dataset, valid_indices)

def limit_dataset_size(dataset, limit_size=4000):
    if len(dataset) <= limit_size:
        return dataset
    indices = torch.randperm(len(dataset))[:limit_size]
    return Subset(dataset, indices)

def load_medmnist_data(data_flag="pneumoniamnist", batch_size=4, limit_size=None, root='./medmnist_data', target_size=128):
    info = INFO[data_flag]
    n_channels = info['n_channels']
    print(f"Количество каналов в датасете {data_flag}: {n_channels}")

    data_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((target_size, target_size)),
        transforms.Normalize(mean=[0.5] * n_channels, std=[0.5] * n_channels)
    ])

    train_dataset = getattr(medmnist, info['python_class'])(
        root=root, split='train', transform=data_transform, download=True
    )
    test_dataset = getattr(medmnist, info['python_class'])(
        root=root, split='test', transform=data_transform, download=True
    )

    print(f"Размер обучающего набора до валидации: {len(train_dataset)}")
    print(f"Размер тестового набора до валидации: {len(test_dataset)}")

    train_dataset = validate_dataset(train_dataset)
    test_dataset = validate_dataset(test_dataset)

    print(f"Размер обучающего набора после валидации: {len(train_dataset)}")
    print(f"Размер тестового набора после валидации: {len(test_dataset)}")

    if len(train_dataset) == 0:
        raise ValueError("Обучающий датасет пустой после валидации. Проверьте загрузку данных.")

    if limit_size is not None:
        original_size = len(train_dataset)
        train_dataset = limit_dataset_size(train_dataset, limit_size)
        print(f"Размер обучающего набора ограничен с {original_size} до {len(train_dataset)} примеров.")

    if len(train_dataset) == 0:
        raise ValueError("Обучающий датасет пустой после ограничения. Увеличьте limit_size или отключите ограничение.")

    if len(train_dataset) < batch_size:
        batch_size = max(1, len(train_dataset) // 2)
        print(f"Размер датасета ({len(train_dataset)}) меньше batch_size. Установлен batch_size={batch_size}.")

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)
    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0)

    train_loader = FilteredDataLoader(train_loader)
    test_loader = FilteredDataLoader(test_loader)

    print(f"Количество батчей в train_loader: {len(train_loader)}")
    print(f"Количество батчей в test_loader: {len(test_loader)}")

    try:
        first_batch = next(iter(train_loader))
        print(f"Размер первого батча: {first_batch[0].size(0)}")
    except StopIteration:
        raise ValueError("train_loader пустой при попытке загрузить первый батч.")

    return train_loader, test_loader, info

def visualize_dataset_samples(dataloader, title="Примеры изображений из набора данных", n_samples=25, figsize=(10, 10)):
    if len(dataloader) == 0:
        print("DataLoader пустой, визуализация невозможна.")
        return None

    images, labels = next(iter(dataloader))
    n_samples = min(n_samples, images.shape[0])
    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=figsize)
    axes = axes.flatten()

    for i in range(n_samples):
        img = images[i].cpu().numpy().transpose((1, 2, 0))
        img = img * 0.5 + 0.5
        axes[i].imshow(img.squeeze(), cmap='gray' if img.shape[2] == 1 else None)
        axes[i].axis('off')

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
    return fig

def save_training_results(generator, fixed_noise, epoch, output_dir, model_name="model"):
    with torch.no_grad():
        generator.eval()
        fake_images = generator(fixed_noise).detach().cpu()
    grid = make_grid(fake_images, normalize=True, nrow=8)
    save_image(grid, f"{output_dir}/{model_name}_epoch_{epoch}.png")
    plt.figure(figsize=(10, 10))
    plt.imshow(grid.permute(1, 2, 0).numpy())
    plt.title(f"Сгенерированные изображения ({model_name}, эпоха {epoch})")
    plt.axis('off')
    plt.show()
    return grid

def plot_losses(losses_dict, model_name, title="Потери модели", figsize=(10, 5)):
    plt.figure(figsize=figsize)
    for label, losses in losses_dict.items():
        plt.plot(losses, label=label)
    plt.xlabel('Итерации')
    plt.ylabel('Потери')
    plt.title(f"{title} ({model_name})")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{RESULTS_DIR}/{model_name}/loss_plot.png")
    plt.show()

## 2. Реализация и обучение моделей

# DCGAN
class DCGANGenerator(nn.Module):
    def __init__(self, nz=100, ngf=64, nc=1, img_size=128, use_instance_norm=False):
        super(DCGANGenerator, self).__init__()
        print(f"Инициализация DCGANGenerator с nc={nc}")
        if nc <= 0:
            raise ValueError(f"nc должно быть больше 0, но получено: {nc}")
        self.nc = nc
        n_upsample = int(np.log2(img_size)) - 2  # Для 128x128 -> 5 слоев
        initial_size = 4
        self.use_instance_norm = use_instance_norm

        model = [
            nn.ConvTranspose2d(nz, ngf * 8, initial_size, 1, 0, bias=False),
        ]
        if use_instance_norm:
            model += [nn.InstanceNorm2d(ngf * 8, affine=True)]
        else:
            model += [nn.GroupNorm(8, ngf * 8)]
        model += [nn.ReLU(True)]

        current_channels = ngf * 8
        for i in range(n_upsample - 1):
            in_channels = current_channels
            out_channels = max(1, ngf * (8 // (2 ** (i + 1))))
            model += [
                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),
            ]
            if use_instance_norm:
                model += [nn.InstanceNorm2d(out_channels, affine=True)]
            else:
                model += [nn.GroupNorm(8, out_channels)]
            model += [nn.ReLU(True)]
            current_channels = out_channels

        model += [
            nn.ConvTranspose2d(current_channels, nc, 4, 2, 1, bias=False),
            nn.Tanh()
        ]

        self.model = nn.Sequential(*model)

    def forward(self, x):
        return self.model(x)

class DCGANDiscriminator(nn.Module):
    def __init__(self, ndf=32, nc=1, img_size=128, use_instance_norm=False):
        super(DCGANDiscriminator, self).__init__()
        print(f"Инициализация DCGANDiscriminator с nc={nc}")
        if nc <= 0:
            raise ValueError(f"nc должно быть больше 0, но получено: {nc}")
        n_downsample = int(np.log2(img_size)) - 2  # 5 слоев
        self.use_instance_norm = use_instance_norm

        model = [
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True)
        ]

        for i in range(n_downsample - 1):
            in_channels = ndf * (2 ** i)
            out_channels = ndf * (2 ** (i + 1))
            model += [
                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),
            ]
            if use_instance_norm:
                model += [nn.InstanceNorm2d(out_channels, affine=True)]
            else:
                model += [nn.GroupNorm(8, out_channels)]
            model += [nn.LeakyReLU(0.2, inplace=True)]

        model += [nn.Conv2d(ndf * (2 ** (n_downsample - 1)), 1, 4, 1, 0, bias=False)]
        self.model = nn.Sequential(*model)

    def forward(self, x):
        x = self.model(x)
        return x.view(-1)

def train_dcgan(train_loader, nz=100, ngf=64, ndf=32, nc=1, img_size=128, num_epochs=30,
                lr=0.0002, beta1=0.5, output_dir=f"{RESULTS_DIR}/dcgan", use_instance_norm=False):
    print(f"[DCGAN] Начало обучения с nc={nc}...")
    start_time = time.time()

    if len(train_loader) == 0:
        raise ValueError("train_loader пустой. Увеличьте limit_size или уменьшите batch_size.")

    netG = DCGANGenerator(nz, ngf, nc, img_size, use_instance_norm).to(device)
    netD = DCGANDiscriminator(ndf, nc, img_size, use_instance_norm).to(device)

    def weights_init(m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            nn.init.normal_(m.weight.data, 0.0, 0.02)
        elif classname.find('GroupNorm') != -1 or classname.find('InstanceNorm2d') != -1:
            if hasattr(m, 'weight') and m.weight is not None:
                nn.init.normal_(m.weight.data, 1.0, 0.02)
            if hasattr(m, 'bias') and m.bias is not None:
                nn.init.constant_(m.bias.data, 0)

    netG.apply(weights_init)
    netD.apply(weights_init)

    criterion = nn.BCEWithLogitsLoss()
    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

    fixed_noise = torch.randn(64, nz, 1, 1, device=device)
    real_label = 1.0
    fake_label = 0.0
    g_losses = []
    d_losses = []

    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        for i, (data, _) in enumerate(tqdm(train_loader, desc=f"Эпоха {epoch+1}/{num_epochs}")):
            if data.size(0) == 0:
                continue

            real_images = data.to(device)
            batch_size = real_images.size(0)
            if batch_size == 0:
                continue

            netD.zero_grad()
            label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)
            output = netD(real_images)
            errD_real = criterion(output, label)
            errD_real.backward()

            noise = torch.randn(batch_size, nz, 1, 1, device=device)
            fake = netG(noise)
            label.fill_(fake_label)
            output = netD(fake.detach())
            errD_fake = criterion(output, label)
            errD_fake.backward()
            errD = errD_real + errD_fake
            optimizerD.step()

            netG.zero_grad()
            label.fill_(real_label)
            output = netD(fake)
            errG = criterion(output, label)
            errG.backward()
            optimizerG.step()

            g_losses.append(errG.item())
            d_losses.append(errD.item())

            if i % 50 == 0:
                print(f"[DCGAN] Эпоха [{epoch+1}/{num_epochs}] Батч {i}/{len(train_loader)} "
                      f"Loss D: {errD.item():.4f}, Loss G: {errG.item():.4f}")

        epoch_time = time.time() - epoch_start_time
        print(f"[DCGAN] Эпоха {epoch+1} завершена за {epoch_time:.2f} секунд.")

        if epoch % 5 == 0 or epoch == num_epochs - 1:
            save_training_results(netG, fixed_noise, epoch, output_dir, "dcgan")
            torch.save({
                'generator': netG.state_dict(),
                'discriminator': netD.state_dict(),
                'g_losses': g_losses,
                'd_losses': d_losses,
                'epoch': epoch,
            }, f"{output_dir}/dcgan_checkpoint_epoch_{epoch}.pt")

    plot_losses({"Generator": g_losses, "Discriminator": d_losses}, "dcgan")
    torch.save({
        'generator': netG.state_dict(),
        'discriminator': netD.state_dict(),
        'g_losses': g_losses,
        'd_losses': d_losses,
        'epoch': num_epochs,
    }, f"{output_dir}/dcgan_final.pt")

    total_time = time.time() - start_time
    print(f"[DCGAN] Обучение завершено за {total_time/60:.2f} минут")
    return netG

# DDPM
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, time_embed_dim, dropout):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.in_layers = nn.Sequential(
            nn.GroupNorm(8, in_channels),
            nn.SiLU(),
            nn.Conv2d(in_channels, out_channels, 3, padding=1)
        )
        self.time_embed_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_embed_dim, out_channels)
        )
        self.out_layers = nn.Sequential(
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Conv2d(out_channels, out_channels, 3, padding=1)
        )
        self.skip_connection = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x, time_emb):
        h = self.in_layers(x)
        time_emb = self.time_embed_layer(time_emb)
        h = h + time_emb.view(-1, time_emb.shape[1], 1, 1)
        h = self.out_layers(h)
        return h + self.skip_connection(x)

class Downsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)

    def forward(self, x, *args):
        return self.conv(x)

class Upsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels, channels, 3, padding=1)

    def forward(self, x, *args):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, in_channels=1, model_channels=64, out_channels=1, num_res_blocks=2,
                 dropout=0.0, channel_mult=(1, 2, 4, 8), time_embed_dim=256):
        super().__init__()
        print(f"Инициализация UNet с in_channels={in_channels}, out_channels={out_channels}")
        self.in_channels = in_channels
        self.model_channels = model_channels
        self.out_channels = out_channels
        self.num_res_blocks = num_res_blocks
        self.dropout = dropout
        self.channel_mult = channel_mult
        self.time_embed_dim = time_embed_dim

        self.time_embed = nn.Sequential(
            nn.Linear(model_channels, time_embed_dim),
            nn.SiLU(),
            nn.Linear(time_embed_dim, time_embed_dim),
        )

        self.input_conv = nn.Conv2d(in_channels, model_channels, 3, padding=1)
        self.down_blocks = nn.ModuleList()
        current_channels = model_channels
        skip_channels_list = []

        for level, mult in enumerate(channel_mult):
            out_channels_block = model_channels * mult
            for i in range(num_res_blocks):
                self.down_blocks.append(ResBlock(current_channels, out_channels_block, time_embed_dim, dropout))
                current_channels = out_channels_block
                if i == num_res_blocks - 1:
                    skip_channels_list.append(current_channels)
            if level != len(channel_mult) - 1:
                self.down_blocks.append(Downsample(current_channels))

        self.middle_block = nn.ModuleList([
            ResBlock(current_channels, current_channels, time_embed_dim, dropout),
            ResBlock(current_channels, current_channels, time_embed_dim, dropout)
        ])

        self.up_blocks = nn.ModuleList()
        for level, mult in reversed(list(enumerate(channel_mult))):
            out_channels_block = model_channels * mult
            for i in range(num_res_blocks):
                skip_channels = skip_channels_list.pop() if i == 0 else 0
                in_channels = current_channels + skip_channels if i == 0 else current_channels
                self.up_blocks.append(ResBlock(in_channels, out_channels_block, time_embed_dim, dropout))
                current_channels = out_channels_block
            if level != 0:
                self.up_blocks.append(Upsample(current_channels))

        self.final_norm = nn.GroupNorm(8, current_channels)
        self.final_conv = nn.Conv2d(current_channels, out_channels, 3, padding=1)

    def forward(self, x, timesteps):
        time_emb = self.get_time_embedding(timesteps)
        time_emb = self.time_embed(time_emb)
        h = self.input_conv(x)
        hs = []
        res_block_count = 0

        for i, module in enumerate(self.down_blocks):
            h = module(h, time_emb)
            if isinstance(module, ResBlock):
                res_block_count += 1
                if res_block_count % self.num_res_blocks == 0:
                    hs.append(h)

        for module in self.middle_block:
            h = module(h, time_emb)

        hs = hs[::-1]
        skip_connections = iter(hs)
        for i, module in enumerate(self.up_blocks):
            if isinstance(module, ResBlock) and i in [0, 3, 6, 9]:
                skip = next(skip_connections)
                if h.shape[2:] != skip.shape[2:]:
                    skip = F.interpolate(skip, size=h.shape[2:], mode='bilinear', align_corners=False)
                h = torch.cat([h, skip], dim=1)
            h = module(h, time_emb)

        h = self.final_norm(h)
        h = F.silu(h)
        h = self.final_conv(h)
        return h

    def get_time_embedding(self, timesteps):
        half_dim = self.model_channels // 2
        emb = np.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
        emb = timesteps.float()[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if self.model_channels % 2 == 1:
            emb = F.pad(emb, (0, 1, 0, 0))
        return emb

class DDPM(nn.Module):
    def __init__(self, denoise_fn, img_size=128, img_channels=1, num_timesteps=1000):
        super().__init__()
        print(f"Инициализация DDPM с img_channels={img_channels}")
        self.denoise_fn = denoise_fn
        self.img_size = img_size
        self.img_channels = img_channels
        self.num_timesteps = num_timesteps

        betas = torch.linspace(1e-4, 0.02, num_timesteps)
        self.register_buffer('betas', betas)
        alphas = 1.0 - betas
        self.register_buffer('alphas', alphas)
        self.register_buffer('alphas_cumprod', torch.cumprod(alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - self.alphas_cumprod))

        posterior_variance = betas * (1.0 - torch.roll(self.alphas_cumprod, 1)) / (1.0 - self.alphas_cumprod)
        posterior_variance[0] = posterior_variance[1]
        self.register_buffer('posterior_variance', posterior_variance)

    def q_sample(self, x_0, t, noise=None):
        if noise is None:
            noise = torch.randn_like(x_0)
        return (self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1) * x_0 +
                self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1) * noise)

    def p_losses(self, x_0, t, noise=None):
        if noise is None:
            noise = torch.randn_like(x_0)
        x_t = self.q_sample(x_0, t, noise)
        predicted_noise = self.denoise_fn(x_t, t)
        loss = F.mse_loss(predicted_noise, noise)
        return loss

    def p_sample(self, x_t, t, t_index):
        predicted_noise = self.denoise_fn(x_t, t)
        alpha = self.alphas[t_index]
        alpha_cumprod = self.alphas_cumprod[t_index]
        beta = self.betas[t_index]
        sigma = torch.sqrt(beta)
        mean = (x_t - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise) / torch.sqrt(alpha)
        if t_index > 0:
            noise = torch.randn_like(x_t)
            return mean + sigma * noise
        return mean

    @torch.no_grad()
    def p_sample_loop(self, shape):
        device = next(self.denoise_fn.parameters()).device
        img = torch.randn(shape, device=device)
        for i in reversed(range(self.num_timesteps)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            img = self.p_sample(img, t, i)
        return img

    @torch.no_grad()
    def sample(self, batch_size=16):
        shape = (batch_size, self.img_channels, self.img_size, self.img_size)
        return self.p_sample_loop(shape)

    def forward(self, x_0, return_loss=True):
        batch_size = x_0.shape[0]
        device = x_0.device
        if return_loss:
            t = torch.randint(0, self.num_timesteps, (batch_size,), device=device).long()
            loss = self.p_losses(x_0, t)
            return loss
        return self.sample(batch_size)

def train_ddpm(train_loader, img_size=128, img_channels=1, model_channels=64, num_timesteps=1000,
               num_epochs=15, lr=1e-4, output_dir=f"{RESULTS_DIR}/ddpm"):
    print(f"[DDPM] Начало обучения с img_channels={img_channels}...")
    start_time = time.time()

    denoise_fn = UNet(in_channels=img_channels, model_channels=model_channels, out_channels=img_channels).to(device)
    model = DDPM(denoise_fn=denoise_fn, img_size=img_size, img_channels=img_channels, num_timesteps=num_timesteps).to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    losses = []

    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        epoch_losses = []

        for i, (data, _) in enumerate(tqdm(train_loader, desc=f"Эпоха {epoch+1}/{num_epochs}")):
            real_images = data.to(device)
            batch_size = real_images.size(0)

            optimizer.zero_grad()
            loss = model(real_images)
            loss.backward()
            optimizer.step()
            epoch_losses.append(loss.item())

            if i % 50 == 0:
                print(f"[DDPM] Эпоха [{epoch+1}/{num_epochs}] Батч {i}/{len(train_loader)} Loss: {loss.item():.4f}")

        avg_loss = sum(epoch_losses) / len(epoch_losses)
        losses.append(avg_loss)
        epoch_time = time.time() - epoch_start_time
        print(f"[DDPM] Эпоха {epoch+1} завершена за {epoch_time:.2f} секунд. Средняя потеря: {avg_loss:.4f}")

        if (epoch + 1) % 5 == 0 or epoch + 1 == num_epochs:
            with torch.no_grad():
                samples = model.sample(batch_size=16)
                samples = (samples + 1) * 0.5
                grid = make_grid(samples, nrow=4, normalize=False)
                save_image(grid, f"{output_dir}/ddpm_epoch_{epoch+1}.png")
                plt.figure(figsize=(8, 8))
                plt.imshow(grid.permute(1, 2, 0).cpu().numpy())
                plt.title(f"DDPM, эпоха {epoch+1}")
                plt.axis('off')
                plt.show()

            torch.save(model.state_dict(), f"{output_dir}/ddpm_checkpoint_epoch_{epoch+1}.pt")

    plot_losses({"DDPM Loss": losses}, "ddpm", "Потери DDPM")
    torch.save(model.state_dict(), f"{output_dir}/ddpm_final.pt")
    total_time = time.time() - start_time
    print(f"[DDPM] Обучение завершено за {total_time/60:.2f} минут")
    return model

# StyleGAN
class MappingNetwork(nn.Module):
    def __init__(self, z_dim=512, w_dim=512, num_layers=8):
        super().__init__()
        layers = []
        for _ in range(num_layers):
            layers.append(nn.Linear(z_dim, z_dim))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
        layers[-1] = nn.Linear(z_dim, w_dim)
        self.net = nn.Sequential(*layers)

    def forward(self, z):
        return self.net(z)

class SynthesisBlock(nn.Module):
    def __init__(self, in_channels, out_channels, w_dim, resolution, upsample=True):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.w_dim = w_dim
        self.resolution = resolution
        self.upsample = upsample

        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.to_rgb = nn.Conv2d(out_channels, 1, 1)
        self.noise_strength = nn.Parameter(torch.zeros([]))
        self.style = nn.Linear(w_dim, in_channels * 2)

    def forward(self, x, w, noise=None):
        if self.upsample:
            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        style = self.style(w)
        gamma, beta = style.chunk(2, dim=1)
        x = self.conv(x)
        if noise is None:
            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)
        x = x + noise * self.noise_strength
        x = F.instance_norm(x)
        x = x * gamma.view(-1, self.out_channels, 1, 1) + beta.view(-1, self.out_channels, 1, 1)
        x = F.leaky_relu(x, 0.2)
        rgb = self.to_rgb(x)
        return x, rgb

class StyleGANGenerator(nn.Module):
    def __init__(self, z_dim=512, w_dim=512, nc=1, img_size=128):
        super().__init__()
        self.z_dim = z_dim
        self.w_dim = w_dim
        self.nc = nc
        self.mapping = MappingNetwork(z_dim, w_dim)
        self.initial_const = nn.Parameter(torch.randn(1, 512, 4, 4))
        resolutions = [2 ** i for i in range(2, int(np.log2(img_size)) + 1)]
        channels = [min(512, 512 // (2 ** (i - 2))) for i in range(2, int(np.log2(img_size)) + 1)]

        self.blocks = nn.ModuleList()
        in_channels = 512
        for i, (res, out_channels) in enumerate(zip(resolutions, channels)):
            upsample = i > 0
            self.blocks.append(SynthesisBlock(in_channels, out_channels, w_dim, res, upsample))
            in_channels = out_channels

    def forward(self, z):
        w = self.mapping(z)
        x = self.initial_const.expand(z.size(0), -1, -1, -1)
        rgb = torch.zeros(z.size(0), self.nc, 4, 4, device=x.device)
        for block in self.blocks:
            x, rgb_new = block(x, w)
            if rgb.shape[2:] != rgb_new.shape[2:]:
                rgb = F.interpolate(rgb, size=rgb_new.shape[2:], mode='bilinear', align_corners=False)
            rgb = rgb + rgb_new
        return rgb

class StyleGANDiscriminator(nn.Module):
    def __init__(self, nc=1, img_size=128):
        super().__init__()
        channels = [min(512, 512 // (2 ** (i - 2))) for i in range(2, int(np.log2(img_size)) + 1)][::-1]
        self.from_rgb = nn.Conv2d(nc, channels[0], 1)
        self.blocks = nn.ModuleList()
        for i in range(len(channels) - 1):
            self.blocks.append(nn.Sequential(
                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(channels[i + 1], channels[i + 1], 3, padding=1),
                nn.LeakyReLU(0.2, inplace=True),
                nn.AvgPool2d(2)
            ))
        self.final = nn.Sequential(
            nn.Conv2d(channels[-1], channels[-1], 4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Flatten(),
            nn.Linear(channels[-1], 1)
        )

    def forward(self, x):
        x = self.from_rgb(x)
        for block in self.blocks:
            x = block(x)
        return self.final(x)

def train_stylegan(train_loader, z_dim=512, nc=1, img_size=128, num_epochs=30,
                   lr=0.002, output_dir=f"{RESULTS_DIR}/stylegan"):
    print(f"[StyleGAN] Начало обучения с nc={nc}...")
    start_time = time.time()

    netG = StyleGANGenerator(z_dim, nc=nc, img_size=img_size).to(device)
    netD = StyleGANDiscriminator(nc=nc, img_size=img_size).to(device)

    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0, 0.99))
    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0, 0.99))

    fixed_noise = torch.randn(64, z_dim, device=device)
    real_label = 1.0
    fake_label = 0.0
    g_losses = []
    d_losses = []

    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        for i, (data, _) in enumerate(tqdm(train_loader, desc=f"Эпоха {epoch+1}/{num_epochs}")):
            if data.size(0) == 0:
                continue

            real_images = data.to(device)
            batch_size = real_images.size(0)
            if batch_size == 0:
                continue

            # Обучение дискриминатора
            netD.zero_grad()
            label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)
            output = netD(real_images)
            errD_real = F.binary_cross_entropy_with_logits(output, label)
            errD_real.backward()

            noise = torch.randn(batch_size, z_dim, device=device)
            fake = netG(noise)
            label.fill_(fake_label)
            output = netD(fake.detach())
            errD_fake = F.binary_cross_entropy_with_logits(output, label)
            errD_fake.backward()
            errD = errD_real + errD_fake
            optimizerD.step()

            # Обучение генератора
            netG.zero_grad()
            label.fill_(real_label)
            output = netD(fake)
            errG = F.binary_cross_entropy_with_logits(output, label)
            errG.backward()
            optimizerG.step()

            g_losses.append(errG.item())
            d_losses.append(errD.item())

            if i % 50 == 0:
                print(f"[StyleGAN] Эпоха [{epoch+1}/{num_epochs}] Батч {i}/{len(train_loader)} "
                      f"Loss D: {errD.item():.4f}, Loss G: {errG.item():.4f}")

        epoch_time = time.time() - epoch_start_time
        print(f"[StyleGAN] Эпоха {epoch+1} завершена за {epoch_time:.2f} секунд.")

        if epoch % 5 == 0 or epoch == num_epochs - 1:
            save_training_results(netG, fixed_noise, epoch, output_dir, "stylegan")
            torch.save({
                'generator': netG.state_dict(),
                'discriminator': netD.state_dict(),
                'g_losses': g_losses,
                'd_losses': d_losses,
                'epoch': epoch,
            }, f"{output_dir}/stylegan_checkpoint_epoch_{epoch}.pt")

    plot_losses({"Generator": g_losses, "Discriminator": d_losses}, "stylegan")
    torch.save({
        'generator': netG.state_dict(),
        'discriminator': netD.state_dict(),
        'g_losses': g_losses,
        'd_losses': d_losses,
        'epoch': num_epochs,
    }, f"{output_dir}/stylegan_final.pt")

    total_time = time.time() - start_time
    print(f"[StyleGAN] Обучение завершено за {total_time/60:.2f} минут")
    return netG

## 3. Метрики оценки

from skimage.metrics import structural_similarity as ssim
from scipy import linalg
import pandas as pd

def calculate_fid(real_features, generated_features):
    mu1 = np.mean(real_features, axis=0)
    sigma1 = np.cov(real_features, rowvar=False)
    mu2 = np.mean(generated_features, axis=0)
    sigma2 = np.cov(generated_features, rowvar=False)
    diff = mu1 - mu2
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)
    return fid

def calculate_ssim_batch(real_images, generated_images):
    real_images = real_images.cpu().numpy()
    generated_images = generated_images.cpu().numpy()
    batch_size = real_images.shape[0]
    ssim_values = []

    for i in range(batch_size):
        real_img = real_images[i].transpose((1, 2, 0))
        gen_img = generated_images[i].transpose((1, 2, 0))
        if real_img.min() < 0:
            real_img = (real_img + 1) / 2
            gen_img = (gen_img + 1) / 2
        if real_img.shape[2] == 1:
            real_img = real_img.squeeze(2)
            gen_img = gen_img.squeeze(2)
        ssim_value = ssim(real_img, gen_img, data_range=1.0, channel_axis=None if real_img.ndim == 2 else 2)
        ssim_values.append(ssim_value)

    return np.mean(ssim_values)

class FeatureExtractor(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, 512),
            nn.ReLU()
        )

    def forward(self, x):
        return self.model(x)

class Classifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.model(x)

def train_classifier(generator, train_loader, test_loader, num_classes, num_synthetic_samples=1000, model_type="dcgan"):
    classifier = Classifier(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(classifier.parameters(), lr=0.001)

    if len(train_loader) == 0:
        raise ValueError("train_loader пустой в train_classifier. Увеличьте limit_size или уменьшите batch_size.")

    real_images = []
    real_labels = []
    for images, labels in train_loader:
        if images.size(0) == 0:
            continue
        real_images.append(images.to(device))
        if labels.dim() > 1:
            labels = labels.squeeze()
        real_labels.append(labels.to(device))
    if not real_images:
        raise ValueError("Не удалось собрать реальные изображения в train_classifier. Проверьте train_loader.")
    real_images = torch.cat(real_images, dim=0)
    real_labels = torch.cat(real_labels, dim=0)

    synthetic_images = []
    synthetic_labels = []
    with torch.no_grad():
        for cls in range(num_classes):
            n_samples = num_synthetic_samples // num_classes
            if model_type == "ddpm":
                images = generator.sample(batch_size=n_samples)
            else:
                noise = torch.randn(n_samples, 100 if model_type == "dcgan" else 512, 1 if model_type == "dcgan" else None, 1 if model_type == "dcgan" else None, device=device)
                images = generator(noise if model_type == "dcgan" else noise.squeeze())
            synthetic_images.append(images)
            labels = torch.full((n_samples,), cls, device=device)
            synthetic_labels.append(labels)

    synthetic_images = torch.cat(synthetic_images, dim=0)
    synthetic_labels = torch.cat(synthetic_labels, dim=0)
    combined_images = torch.cat([real_images, synthetic_images], dim=0)
    combined_labels = torch.cat([real_labels, synthetic_labels], dim=0)

    classifier.train()
    batch_size = 4
    n_samples = combined_images.shape[0]

    for epoch in range(10):
        idx = torch.randperm(n_samples)
        combined_images = combined_images[idx]
        combined_labels = combined_labels[idx]
        for i in range(0, n_samples, batch_size):
            batch_images = combined_images[i:i+batch_size]
            batch_labels = combined_labels[i:i+batch_size]
            if batch_images.size(0) == 0:
                continue
            optimizer.zero_grad()
            outputs = classifier(batch_images)
            loss = criterion(outputs, batch_labels)
            loss.backward()
            optimizer.step()

    classifier.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            if labels.dim() > 1:
                labels = labels.squeeze()
            labels = labels.to(device)
            if images.size(0) == 0:
                continue
            outputs = classifier(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total if total > 0 else 0
    return accuracy

def evaluate_model(generator, train_loader, test_loader, dataset_info, num_samples=1000, model_type="dcgan"):
    if len(train_loader) == 0:
        raise ValueError(f"train_loader пустой в evaluate_{model_type}. Увеличьте limit_size или уменьшите batch_size.")

    real_images = []
    with torch.no_grad():
        for images, _ in train_loader:
            if images.size(0) == 0:
                continue
            real_images.append(images)
            if len(real_images) * images.shape[0] >= num_samples:
                break
    if not real_images:
        raise ValueError(f"Не удалось собрать реальные изображения для оценки {model_type}. Проверьте train_loader.")
    real_images = torch.cat(real_images, dim=0)[:num_samples]

    feature_extractor = FeatureExtractor().to(device)
    real_features = []
    with torch.no_grad():
        for i in range(0, num_samples, 4):
            batch = real_images[i:i+4].to(device)
            if batch.size(0) == 0:
                continue
            features = feature_extractor(batch)
            real_features.append(features.cpu().numpy())
    real_features = np.concatenate(real_features, axis=0)

    generated_images = []
    with torch.no_grad():
        if model_type == "ddpm":
            images = generator.sample(batch_size=num_samples)
            generated_images.append(images)
        else:
            for i in range(0, num_samples, 4):
                batch_size = min(4, num_samples - i)
                noise = torch.randn(batch_size, 100 if model_type == "dcgan" else 512, 1 if model_type == "dcgan" else None, 1 if model_type == "dcgan" else None, device=device)
                images = generator(noise if model_type == "dcgan" else noise.squeeze())
                generated_images.append(images)
    generated_images = torch.cat(generated_images, dim=0)[:num_samples]

    grid = make_grid(generated_images[:64], nrow=8, normalize=True)
    plt.figure(figsize=(10, 10))
    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())
    plt.title(f"Примеры изображений, сгенерированных {model_type.upper()}")
    plt.axis('off')
    plt.show()

    generated_features = []
    with torch.no_grad():
        for i in range(0, num_samples, 4):
            batch = generated_images[i:i+4].to(device)
            if batch.size(0) == 0:
                continue
            features = feature_extractor(batch)
            generated_features.append(features.cpu().numpy())
    generated_features = np.concatenate(generated_features, axis=0)

    fid = calculate_fid(real_features, generated_features)
    ssim_value = calculate_ssim_batch(real_images[:100], generated_images[:100])
    accuracy = train_classifier(generator, train_loader, test_loader, len(dataset_info['label']), model_type=model_type)

    results = {
        'FID': fid,
        'SSIM': ssim_value,
        'Accuracy': accuracy
    }
    print(f"[{model_type.upper()}] FID: {fid:.4f} (ниже - лучше)")
    print(f"[{model_type.upper()}] SSIM: {ssim_value:.4f} (выше - лучше)")
    print(f"[{model_type.upper()}] Точность классификатора: {accuracy:.2f}%")
    return results

def evaluate_all_models(dcgan_gen, ddpm_model, stylegan_gen, train_loader, test_loader, dataset_info, num_samples=1000):
    results = {}
    results['dcgan'] = evaluate_model(dcgan_gen, train_loader, test_loader, dataset_info, num_samples, "dcgan")
    results['ddpm'] = evaluate_model(ddpm_model, train_loader, test_loader, dataset_info, num_samples, "ddpm")
    results['stylegan'] = evaluate_model(stylegan_gen, train_loader, test_loader, dataset_info, num_samples, "stylegan")

    df = pd.DataFrame([
        ['DCGAN', results['dcgan']['FID'], results['dcgan']['SSIM'], results['dcgan']['Accuracy']],
        ['DDPM', results['ddpm']['FID'], results['ddpm']['SSIM'], results['ddpm']['Accuracy']],
        ['StyleGAN', results['stylegan']['FID'], results['stylegan']['SSIM'], results['stylegan']['Accuracy']]
    ], columns=['Model', 'FID', 'SSIM', 'Accuracy'])
    print("\nСравнение результатов:")
    print(df)
    df.to_csv(f"{RESULTS_DIR}/evaluation/all_models_results.csv", index=False)
    return results

## 4. Интерфейс на Gradio

import gradio as gr

def create_gradio_interface(dcgan_gen, ddpm_model, stylegan_gen):
    def generate_images(model_choice, num_images):
        with torch.no_grad():
            if model_choice == "DCGAN":
                noise = torch.randn(num_images, 100, 1, 1, device=device)
                images = dcgan_gen(noise)
            elif model_choice == "DDPM":
                images = ddpm_model.sample(batch_size=num_images)
            elif model_choice == "StyleGAN":
                noise = torch.randn(num_images, 512, device=device)
                images = stylegan_gen(noise)
            images = (images + 1) / 2
            images = images.clamp(0, 1)
            grid = make_grid(images, nrow=int(np.sqrt(num_images)), padding=2, normalize=False)
            grid = grid.cpu().permute(1, 2, 0).numpy()
            if grid.shape[2] == 1:
                grid = grid.squeeze(2)
            return grid

    with gr.Blocks(title="Система аугментации медицинских изображений") as demo:
        gr.Markdown("# Система аугментации медицинских изображений")
        gr.Markdown("Выберите модель и количество изображений для генерации.")
        with gr.Row():
            with gr.Column():
                model_dropdown = gr.Dropdown(choices=["DCGAN", "DDPM", "StyleGAN"], label="Модель", value="DCGAN")
                num_images_slider = gr.Slider(minimum=1, maximum=64, step=1, value=16, label="Количество изображений")
                generate_button = gr.Button("Сгенерировать изображения")
            with gr.Column():
                output_image = gr.Image(label="Сгенерированные изображения")
        generate_button.click(fn=generate_images, inputs=[model_dropdown, num_images_slider], outputs=output_image)
    demo.launch(share=True)

## 5. Обучение и оценка

def main(limit_size=None, use_instance_norm=False):
    if not torch.cuda.is_available():
        raise RuntimeError("Этот код требует GPU для работы. Пожалуйста, используйте Google Colab с GPU.")
    print(f"Используется GPU: {torch.cuda.get_device_name(0)}")

    data_flag = "pneumoniamnist"
    batch_size = 4
    target_size = 128

    print(f"Загрузка данных {data_flag}...")
    train_loader, test_loader, dataset_info = load_medmnist_data(data_flag, batch_size, limit_size=limit_size, root=DATA_DIR, target_size=target_size)
    if len(train_loader) == 0:
        raise ValueError("train_loader пустой. Увеличьте limit_size или уменьшите batch_size.")
    print(f"Количество батчей в train_loader: {len(train_loader)}")
    visualize_dataset_samples(train_loader, title=f"Примеры изображений из набора данных {data_flag}")

    img_size = target_size
    img_channels = dataset_info['n_channels']
    print(f"img_channels перед передачей в модели: {img_channels}")

    print("\n" + "="*50)
    print("Обучение DCGAN")
    print("="*50)
    dcgan_generator = train_dcgan(train_loader, nz=100, ngf=64, ndf=32, nc=img_channels, img_size=img_size, num_epochs=30, use_instance_norm=use_instance_norm)

    print("\n" + "="*50)
    print("Обучение DDPM")
    print("="*50)
    ddpm_model = train_ddpm(train_loader, img_size=img_size, img_channels=img_channels, model_channels=64, num_timesteps=1000, num_epochs=15)

    print("\n" + "="*50)
    print("Обучение StyleGAN")
    print("="*50)
    stylegan_generator = train_stylegan(train_loader, z_dim=512, nc=img_channels, img_size=img_size, num_epochs=30)

    print("\n" + "="*50)
    print("Оценка всех моделей")
    print("="*50)
    evaluation_results = evaluate_all_models(dcgan_generator, ddpm_model, stylegan_generator, train_loader, test_loader, dataset_info, num_samples=1000)

    print("\n" + "="*50)
    print("Создание интерфейса Gradio")
    print("="*50)
    create_gradio_interface(dcgan_generator, ddpm_model, stylegan_generator)

    print("\n" + "="*50)
    print("Обучение и оценка завершены!")
    print("="*50)
    return dcgan_generator, ddpm_model, stylegan_generator, evaluation_results

if __name__ == "__main__":
    use_limited_dataset = False
    limit_size = 4000 if use_limited_dataset else None
    dcgan_gen, ddpm_model, stylegan_gen, evaluation_results = main(limit_size=limit_size, use_instance_norm=True)

